{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWa2nHPPhVXP"
   },
   "source": [
    "# SIR epidemic model - [data assimilation]\n",
    "\n",
    "This notebook is the second part of inspecting SIR epidemic model. \n",
    "Here we will perform epidemic data assimilation with proposed numerical model using manually implemented 4D-Var method in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxcqgv3UhVXb"
   },
   "source": [
    "## 4D-Var method\n",
    "\n",
    "Data assimilation is the process of optimizing the fit between gathered data and some prior (theoretical or based on previous measure).\n",
    "\n",
    "`4D-Var` is one of the methods for data assimilation - it widely used for weather forecast as a successor of `3D-Var` method. \n",
    "\n",
    "<img src=\"https://i.ibb.co/sWLCYSK/Andersson-NL115-figure-1-box-754px.png\" width=\"400\">\n",
    "\n",
    "`4D-Var` loss for $x$ model parameters is defined:\n",
    "\n",
    "$$J(x) = (x-x_b)^T \\boldsymbol{B}^{-1} (x-x_b) + \\sum_{i=0}^{n}(y_i-\\mathcal{H}_i(x_i))^T \\boldsymbol{R}_i^{-1} (y_i-\\mathcal{H}_i(x_i))$$\n",
    "\n",
    "where: \n",
    "- $x_b$ - our prior parameters\n",
    "- $\\boldsymbol{B}$ - a background error covariance\n",
    "- $\\boldsymbol{R}$ - a observational error covariance\n",
    "- $y$ - observation data\n",
    "- $\\mathcal{H}$ - operator that transforms input space vector into output space\n",
    "\n",
    "`3D-Var` method assumed that all observed data happened at one fixed point of time for given assimilation window. So when performing DA for weather conditions four times a day a six hours  time span is considered as one point in time. As we know weather conditions can change through time so `4D-Var` introduced time dimension to count in temporal changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8g1_KGGChVXh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5KGi_B4hVYO"
   },
   "outputs": [],
   "source": [
    "assert tf.__version__[0] == '2', 'Tensorflow 2.x version required'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUjsaU8xhVYq"
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjdyKip-hVZN"
   },
   "source": [
    "### Task 1: Complete equations according to formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMz1X2TnhVZR"
   },
   "outputs": [],
   "source": [
    "def deriv(y, N, beta, gamma):\n",
    "    S, I, R = y\n",
    "    dSdt = ???\n",
    "    dIdt = ???\n",
    "    dRdt = ???\n",
    "    return dSdt, dIdt, dRdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI7wC9V_hVZv"
   },
   "source": [
    "### Task 2: Complete the formula for each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTaglbljhVZ0"
   },
   "outputs": [],
   "source": [
    "def euler(deriv, y0, dt, steps, N, beta, gamma):\n",
    "    S0, I0, R0 = y0\n",
    "    S_result, I_result, R_result = S0, I0, R0\n",
    "    for i in range(steps-1):\n",
    "        S1, I1, R1 = ???\n",
    "        next_s, next_i, next_r = ???\n",
    "        S_result = tf.concat([S_result, next_s], axis=0)\n",
    "        I_result = tf.concat([I_result, next_i], axis=0)\n",
    "        R_result = tf.concat([R_result, next_r], axis=0)\n",
    "        S0, I0, R0 = next_s, next_i, next_r\n",
    "    return S_result, I_result, R_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuyvGHZuhVaV"
   },
   "outputs": [],
   "source": [
    "# Total population\n",
    "N = tf.constant([2500.])\n",
    "\n",
    "# Initial infected and recovered individuals\n",
    "I0, R0 = tf.constant([1.]), tf.constant([0.])\n",
    "\n",
    "# Everyone else, S0, is susceptible to infection initially\n",
    "S0 = N - I0 - R0\n",
    "\n",
    "# Initial conditions tuple\n",
    "y0 = S0, I0, R0\n",
    "\n",
    "# A grid of time points (in days)\n",
    "days = 160\n",
    "t = np.linspace(0, days, days)\n",
    "dt = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSFqca1_hVbQ"
   },
   "outputs": [],
   "source": [
    "prior = tf.constant([[0.3],[0.1]])\n",
    "trainable_weights = tf.Variable([[0.2],[0.1]]) # posterior\n",
    "B_inv = tf.eye(2) * 100000.\n",
    "R_inv = tf.eye(1) * 0.00005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0lRyt0qhVbj"
   },
   "outputs": [],
   "source": [
    "def model_I(deriv, y0, dt, steps, N, beta, gamma):\n",
    "    _, I, _ = euler(deriv, y0, dt, steps, N, beta, gamma)\n",
    "    return I[:, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0Hz7awYhVb5"
   },
   "outputs": [],
   "source": [
    "def model_x(x_pred, horizon=70):\n",
    "    return model_I(deriv, y0, dt, days, N, x_pred[0,0], x_pred[1,0])[:horizon]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Mg0o1WlCjR6v",
    "outputId": "8528b4c0-4a09-48cf-f65e-394dc0d5adbb"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('New Zealand COVID active cases.csv', delimiter=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPO4pbYWhVcM"
   },
   "outputs": [],
   "source": [
    "raw = model_x(trainable_weights, horizon=days)\n",
    "y = data.loc[:, 'Active cases'].values\n",
    "y = y[:, None, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "br8UGDzwhVdX"
   },
   "source": [
    "### Task 3: Complete the loss function according to the formula\n",
    "\n",
    "Hint: You should use `tf.transpose` and `tf.reduce_sum`. In `tf.transpose` for observation part notice that first dimension is batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAxRHqUmhVdc"
   },
   "source": [
    "`4D-Var` loss for $x$ model parameters is:\n",
    "\n",
    "$$J(x) = (x-x_b)^T \\boldsymbol{B}^{-1} (x-x_b) + \\sum_{i=0}^{n}(y_i-\\mathcal{H}_i(x_i))^T \\boldsymbol{R}_i^{-1} (y_i-\\mathcal{H}_i(x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxYBwW9AhVdg"
   },
   "source": [
    "With exemplary substitution it would be:\n",
    "\n",
    "\\begin{gather}\n",
    "J(x) = \n",
    "\\begin{bmatrix} x_{0} & x_{1} \\end{bmatrix} \n",
    "\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n",
    "\\begin{bmatrix} x_{0} \\\\ x_{1} \\end{bmatrix}\n",
    "      +\n",
    "\\sum_{i=0}^{n}\n",
    "\\begin{bmatrix} y_{0}^i & y_{1}^i \\end{bmatrix} \n",
    "\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n",
    "\\begin{bmatrix} y_{0}^i \\\\ y_{1}^i \\end{bmatrix}\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJqhn7V_hVdj"
   },
   "outputs": [],
   "source": [
    "# (batch_size, rows, cols)\n",
    "@tf.function\n",
    "def custom_loss(prior, x_pred):\n",
    "    custom_loss = ???\n",
    "    return custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_iters = 200\n",
    "penalty = np.zeros(num_iters, np.float64)\n",
    "\n",
    "for i in range(num_iters):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = custom_loss(prior, trainable_weights)\n",
    "    grads = tape.gradient(loss, trainable_weights)\n",
    "    opt.apply_gradients([(grads, trainable_weights)])\n",
    "    penalty[i] = loss\n",
    "    if i % 10 == 0:\n",
    "        print(f'Loss for {i}th iteration: ', loss.numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(custom_loss.pretty_printed_concrete_signatures())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEDJBrBShVeS"
   },
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "QZ8onHV8hVeU",
    "outputId": "34033b88-315d-4b74-d338-16d392c5e432",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, tf.squeeze(model_x(prior, horizon=days)))\n",
    "ax.plot(t, tf.squeeze(model_x(trainable_weights, horizon=days)), color='r')\n",
    "ax.plot(t, tf.squeeze(raw), color='g')\n",
    "ax.scatter(t[:70], tf.squeeze(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: What is the number of cases for day no. 80?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMAoZ4BmhVer"
   },
   "source": [
    "### Task 5: Perform epidemic prediction, find params that won't exceed health system capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "SA1MqkNlhVet",
    "outputId": "1ea57282-96cf-47d9-8569-8c2741b033e8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "health_sys_cap = 600\n",
    "ax.plot(t, tf.squeeze(model_x(trainable_weights, horizon=days)), color='r')\n",
    "ax.plot(t, [health_sys_cap for _ in t], '--k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5t6_w0-WhVe4",
    "outputId": "89dcdbd0-7e22-4a1d-ad03-b45231ce5a8d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Lab-part-2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
